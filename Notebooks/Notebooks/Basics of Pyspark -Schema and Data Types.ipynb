{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "973dec63-35b5-4528-93e5-aa02d13c746c",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "#### Schema\n",
    "In PySpark, a schema is like a blueprint that describes the structure of your data. \n",
    "It defines what kind of information each part of your data contains. The schema helps PySpark understand how to organize and process the data, making it easier to work with and analyze. \n",
    "It's like having a map that guides PySpark in navigating and making sense of the information within your dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "ad60e272-1253-465d-aae9-3703ff1273d1",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "#### Main data types in PySpark\n",
    "* **float**: Continuous number, with decimals.\n",
    "* **integer**: Discrete number. \n",
    "* **string**: Textual information\n",
    "* **boolean**: True or False type\n",
    "* **timestamp**: Datetime type\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0196a8a2-35d9-4e66-a4d2-a752917aefde",
     "showTitle": true,
     "title": "Loading a DataFrame"
    }
   },
   "outputs": [],
   "source": [
    "path = '/databricks-datasets/Rdatasets/data-001/csv/ggplot2/diamonds.csv'\n",
    "df = spark.read.csv(path, header=True, inferSchema=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c73e2ff4-6264-42e9-8f11-52e269645ce8",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "display(df.limit(2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3d7259e0-026e-4cf7-bac4-80f69bd0e97c",
     "showTitle": true,
     "title": "Schema"
    }
   },
   "outputs": [],
   "source": [
    "# Importing Struct Types\n",
    "from pyspark.sql.types import DoubleType, StringType, StructField, StructType, IntegerType\n",
    "from pyspark.sql import functions as F\n",
    " \n",
    " # StructType is a list of StrctFields\n",
    "schema = StructType([\n",
    "# A StructFielad takes in the name of the variable, \n",
    "# the type and a boolean indicating if null is accepted or not.\n",
    "  StructField(\"_c0\", StringType(), False),\n",
    "  StructField(\"carat\", DoubleType(), False),\n",
    "  StructField(\"cut\", StringType(), False),\n",
    "  StructField(\"color\", StringType(), False),\n",
    "  StructField(\"clarity\", StringType(), False),\n",
    "  StructField(\"depth\", DoubleType(), False),\n",
    "  StructField(\"table\", IntegerType(), False),\n",
    "  StructField(\"price\", IntegerType(), False),\n",
    "  StructField(\"x\", DoubleType(), False),\n",
    "  StructField(\"y\", DoubleType(), False),\n",
    "  StructField(\"z\", DoubleType(), False)\n",
    "])\n",
    "\n",
    "# Reload the DF with the schema applied\n",
    "df = spark.read.format(\"csv\").schema(schema).load(path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "e0e1e59b-e6cb-4f3e-ad05-a42177b145c0",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "### Creating Data Frames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "dbf9eb36-4811-4bf7-b25a-db5071cd3a02",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.types import StructType, StructField, StringType, DoubleType\n",
    "\n",
    "my_data = [(\"2023-01-01\",\"PPDQ1\",\"A\",366.3),\n",
    "        (\"2023-01-02\",\"PPDQ2\",\"A\",389.5),\n",
    "        (\"2023-01-02\",\"PPDQ1\",\"B\",289.78),\n",
    "        (\"2023-01-03\",\"PPDQ6\",\"A\",367.45),\n",
    "        (\"2023-01-04\",\"PPDQ3\",\"A\",766.45),\n",
    "        (\"2023-01-04\",\"PPDQ3\",\"B\",703.7),\n",
    "        (\"2023-01-05\",\"PPDQ3\",\"A\",426.74)\n",
    "  ]\n",
    "\n",
    "schema = StructType([ \\\n",
    "    StructField(\"date\",StringType(),True), \\\n",
    "    StructField(\"product\",StringType(),True), \\\n",
    "    StructField(\"store\",StringType(),True), \\\n",
    "    StructField(\"total\", DoubleType(), True)  ])\n",
    " \n",
    "sales = spark.createDataFrame(data=my_data,schema=schema)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "772d654d-dc5c-43f2-a73c-aa9775400935",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "display(sales)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "809fed3d-bd92-43d4-be8a-c02100de032a",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "#### Date and Time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7ac5b595-e836-4481-8a6e-ddbff8d358d8",
     "showTitle": true,
     "title": "variable to date type"
    }
   },
   "outputs": [],
   "source": [
    "# column \"date\" to date type\n",
    "sales = (\n",
    "    sales #dataset\n",
    "    .select(F.to_date(col('date'), 'yyyy-mm-dd').alias('date'),\n",
    "                     'product', 'store', 'total' )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b96b719a-acd1-4137-8d86-f62edc1a7c6e",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "display(sales.limit(2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1d33e312-d440-419a-85ba-5fbc1c48c2f4",
     "showTitle": true,
     "title": "extract Day, Week, Month"
    }
   },
   "outputs": [],
   "source": [
    "(sales\n",
    " .select('date',\n",
    "             F.day('date').alias('day'),\n",
    "             F.weekofyear('date').alias('week'),\n",
    "             F.month('date').alias('mth'))\n",
    " .display()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a96b60d3-8ae0-4f43-9b8c-485cbdaf1393",
     "showTitle": true,
     "title": "Operations with date"
    }
   },
   "outputs": [],
   "source": [
    "display(\n",
    "    sales #dataset\n",
    "    .withColumn('today', F.current_date()) # current system date\n",
    "    .withColumn('today_tmstp', F.current_timestamp()) #add current timestamp\n",
    "    .withColumn('date+10', F.date_add(col('date'), 10)) # add 10 days to date\n",
    "    .withColumn('date_difference', F.date_diff(col('today'), col('date') ) ) #date difference\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "d633cb3e-c999-424e-95eb-5e7d79661560",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "dashboards": [],
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "Basics of Pyspark -Schema and Data Types",
   "widgets": {}
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
